{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "newsgroups = sklearn.datasets.fetch_20newsgroups_vectorized()\n",
    "X, y = newsgroups.data, newsgroups.target\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 1)\n",
    "knn.fit(X, y)\n",
    "y_pred = knn.predict(X)\n",
    "knn.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "knn.fit(X_train, y_train)\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**APPLYTING LOGISTIC REGRESSION AND SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLYING LOGISTIC REGRESSION\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.predict(X_test)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example using Wine Dataset\n",
    "import sklearn.datasets\n",
    "wine = sklearn.datasets.load_wine()\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(wine.data, wine.target)\n",
    "print(lr.score(wine.data, wine.target))\n",
    "\n",
    "#Determining the confidence scores\n",
    "print(lr.predict_proba(wine.data[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASIC SVM CLASSIFIER\n",
    "\n",
    "import sklearn.datasets \n",
    "wine = sklearn.datasets.load_wine()\n",
    "from sklearn.svm import LinearSVC\n",
    "# Using Linear Classifier\n",
    "svm = LinearSVC() #Using Default Hyperparameter\n",
    "svm.fit(wine.data, wine.target)\n",
    "print(svm.score(wine.data, wine.target))\n",
    "\n",
    "#Using NonLinear Classifier\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC() #Using Default Hyperparameter\n",
    "svm.fit(wine.data, wine.target)\n",
    "print(svm.score(wine.data, wine.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter is a choice about the model made by the user before fitting the model\n",
    "It controls the complexity of the model\n",
    "\n",
    "**UNDERFITTING:**\n",
    "If the model is too simple, it maybe unable to capture the patterns in the data leading to low training accuracy\n",
    "\n",
    "**OVERFITTING:**\n",
    "If the model is too complex, it may learn the peculiarities of the training set leading to lower test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "LINEAR DESCISION BOUNDARY\n",
    "\n",
    "- A descision boundary tells us what class out classifier will predict for any value of X\n",
    "- The dividing line between the features is called the descision boundary\n",
    "- Linear Descision Boundary - Descision Boundary is a straight line\n",
    "- Non-Linear Descision Boundary - Descision Boundary is not a straight line and is in the form of a curve\n",
    "\n",
    "Vocabulary:\n",
    "\n",
    "- Classification: It is supervised learning when the y values(target values) are categories \n",
    "- Regression: It is supervised learning when the y values(target values) are trying to predict a continuous value Linearly Seperable - Dataset: It can be perfectly classified by a linear descision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LINEAR CLASSIFIERS\n",
    "\n",
    "The sum of all the elements of the dot product of two matrices can be simply done using the @ symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.arange(3)\n",
    "print(X)\n",
    "y = np.arange(3, 6)\n",
    "print(y)\n",
    "print(\"The dot product of two matrices is: \", X*y)\n",
    "print(\"The sum of the dot product of two matrices is: \", np.sum(X*y))\n",
    "print(\"The sum of the dot product of two matrices using @: \", X@y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LINEAR CLASSIFIER PREDICTION\n",
    "\n",
    "( raw model output ) = ( Coeffectients * Features ) + Intercept [Equation of a line -> y = mx + c]\n",
    "- We then check if the raw model output is positive or negative If it is positive -> predict one class If it is negative -> predict other class\n",
    "- This pattern is the same for Logistic Regression and Linear SVM's\n",
    "- In scikit learn terms, we can say that Logistic Regression and Linear SVM's have different fit function but the same predict functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTATION OF LINEAR CLASSIFIER PREDICTION IN BREAST CANCER CLASSIFIER DETECTION\n",
    "\n",
    "import sklearn.datasets\n",
    "breast_cancer = sklearn.datasets.load_breast_cancer()\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(breast_cancer.data, breast_cancer.target)\n",
    "print(lr.predict(breast_cancer.data)[10]) # predicting the value for 10th index\n",
    "print(lr.predict(breast_cancer.data)[20]) # predicting the value for 20th index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get the learn coefficient and intercept using lr.coef_ and lr.intercept_\n",
    "\n",
    "print(\"Raw Model Output for the 10th Index: \", lr.coef_ @ breast_cancer.data[10] + lr.intercept_)  # This will return the raw model output for the 10th index\n",
    "\n",
    "print(\"Raw Model Output for the 20th Index: \", lr.coef_ @ breast_cancer.data[20] + lr.intercept_)  # This will return the raw model output for the 10th index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOSS FUNCTIONS**\n",
    "\n",
    "- Machine learning algorithms involve minimising loss\n",
    "- Minimising loss is changing the values of coefficitient such that the loss function is minimal\n",
    "- The loss function is a penalty score that assesses how poorly the model is doing on the training data\n",
    "- .score() function is not the loss function\n",
    "\n",
    "i) Least Squares: Squared loss\n",
    "\n",
    "- it mininises the sum of squares of the errors on the training sets\n",
    "- Error = True Target Value - Predicted Target Value\n",
    "- It is not appropriate for classification problems because the y values are categories and not numbers\n",
    "\n",
    "**CLASSIFICATION ERRORS**\n",
    "\n",
    "0-1 LOSS:\n",
    "- A natural loss for classification is the number of errors it makes. This is the 0-1 loss function\n",
    "- Loss function = 0, if the prediction is correct = 1, if the prediction is incorrect\n",
    "- By summing this function for all the training examples, we get the total number of errors made by the model\n",
    "- This loss is very hard to minimise which is why it is not used in logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MINIMIZING LOSS FUNCTIONS USING SCIPY.OPTIMIZE.MINIMISE\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "print(minimize(np.square, 0).x)  \n",
    "# the second argument is our initial guess\n",
    "#.x at the end grabs the input value that makes the function as small as possible\n",
    "print(minimize(np.square, 2).x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTING LINEAR REGRESSION FROM SCRATCH\n",
    "\n",
    "# The squared error, summed over training examples\n",
    "def my_loss(w):\n",
    "    s = 0\n",
    "    for i in range(y.size):\n",
    "        # Get the true and predicted target values for example 'i'\n",
    "        y_i_true = y[i]\n",
    "        y_i_pred = w@X[i]\n",
    "        s = s + (y_i_true - y_i_pred)**2\n",
    "    return s\n",
    "\n",
    "# Returns the w that makes my_loss(w) smallest\n",
    "w_fit = minimize(my_loss, X[0]).x\n",
    "print(w_fit)\n",
    "\n",
    "# Compare with scikit-learn's LinearRegression coefficients\n",
    "lr = LinearRegression(fit_intercept=False).fit(X,y)\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOSS FUNCTION DIAGRAMS**\n",
    "\n",
    "1) 0-1 LOSS FUNCTION:\n",
    "\n",
    "![Screenshot 2024-04-05 190737](Screenshot%202024-04-05%20190737.png)\n",
    "\n",
    "2) LINEAR REGRESSION LOSS DIAGRAM:\n",
    "- This is a quadratic curve as it takes the sum of squares of all errors\n",
    "![Screenshot 2024-04-05 190905](Screenshot%202024-04-05%20190905.png)\n",
    "\n",
    "3) LOGISTIC LOSS:\n",
    "- Used in logistic regression\n",
    "- Smooth version of the 0-1 loss\n",
    "- As we move towards the left, as the value of raw model output decreases, the loss also increases\n",
    "- As we move towards the right, as the values of the raw model output decreases, the loss also decreases\n",
    "![Screenshot 2024-04-05 191214](Screenshot%202024-04-05%20191214.png)\n",
    "\n",
    "4) HINGE LOSS:\n",
    "- Used in SVM's\n",
    "![Screenshot 2024-04-05 191435](Screenshot%202024-04-05%20191435.png)\n",
    "- General shape is similar to Logistic Loss\n",
    "![Screenshot 2024-04-05 191517](Screenshot%202024-04-05%20191517.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "REGULARIZED LOGISTIC REGRESSION\n",
    "\n",
    "- Regularization combats overfitting by making the model coefficients smaller\n",
    "- hyperparameter C is the inverse of regularization strength (i.e. Larger C means less regularization and smaller C means more regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW DOES REGULARIZATION AFFECT TRAINING ACCURACY?\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "breast_cancer = sklearn.datasets.load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "lr_weak_reg = LogisticRegression(C=100)\n",
    "lr_strong_reg = LogisticRegression(C=0.01)\n",
    "\n",
    "lr_weak_reg.fit(X_train, y_train)\n",
    "lr_strong_reg.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training accuracy of model with weaker regularization: \", lr_weak_reg.score(X_train, y_train))\n",
    "print(\"Training accuracy of model with strong regularization: \", lr_strong_reg.score(X_train, y_train))\n",
    "\n",
    "print(\"Test accuracy of model with weaker regularization: \", lr_weak_reg.score(X_test, y_test))\n",
    "print(\"Test accuracy of model with strong regularization: \", lr_strong_reg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REGULARIZATION**\n",
    "\n",
    "- Regularization is an extra term added to the loss function that penalizes the large values of the coeffecients Screenshot 2024-04-06 153204\n",
    "\n",
    "- Without regularization, we are maximising the training accuracy\n",
    "\n",
    "- With regularization, large model coeffecients are penalized which distracts from the goal of high training accuracy. Thus the training accuracy decreases\n",
    "\n",
    "- The larger the regularization penalty(i.e. the smaller the values of C) the more we deviate from the goal of maximising training accuracy.\n",
    "\n",
    "- However, regularization improves the test accuracy\n",
    "\n",
    "- Ridge and Lasso are two different types of regularization\n",
    "\n",
    "- Lasso = Linear Regression with L1 Regularization\n",
    "\n",
    "- Ridge = Linear Regression with L2 Regularization Screenshot 2024-04-06 154055"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training two different Logistic Regression models with L1 and L2 Regularization\n",
    "\n",
    "lr_L1 = LogisticRegression(solver = 'liblinear', penalty = 'l1')\n",
    "lr_L2 = LogisticRegression() # Default is l2 penalty\n",
    "\n",
    "lr_L1.fit(X_train, y_train)\n",
    "lr_L2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION PROBABILITIES\n",
    "\n",
    "- Instead of classifying features as class 1 or class 2, we see the probabilities of it being in either of the classes\n",
    "Screenshot 2024-04-06 155847\n",
    "\n",
    "- The probabilities are computed from the raw model output\n",
    "- The raw model output is squashed to values between 0 and 1 using the sigmoid function\n",
    "Screenshot 2024-04-06 160110"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "MULTI CLASS LOGITIC REGRESSION\n",
    "\n",
    "One vs Rest Strategy:\n",
    "- Comining Multiple Binary Classifiers y==0 returns an array the same size as y where each element is TRUE if the class is 0 and false if the class is not 0 lr0.fit(X, y==0) lr1.fit(X, y==1) lr2.fit(X, y==2)\n",
    "- To make a prediction, we take the class whose classifier gives the largest raw model output(descision_function(X) in scikilearn)\n",
    "- This means that the model is more confident that the class is 0 than any other classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUPPORT VECTORS\n",
    "\n",
    "- Linear SVM's are also Linear Classifiers but they use the hinge loss functionand L2 regularization\n",
    "- Support vectors are defined as training examples that influence the decision boundary.\n",
    "- If a training example falls in the 0 loss region, it does not contribute to the fit.\n",
    "- Support vectors are training examples that are not in the flat part of the loss diagram(do not have loss=0)\n",
    "- Support Vecotrs: Include Incorrectly classified examples and correctly classified examples close to the boundary. The closeness to - the boundary is controlled by the regularization strength\n",
    "- if an example is not a support vector, removing it has no effect on the model\n",
    "- SVM maximises the margin for linearly seperable datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KERNEL SVM's**\n",
    "\n",
    "- Classifying non linear datasets using linear classifiers\n",
    "- Fitting a linear model in the transformed space corresponds to fitting a non linear model in the non transformed space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(gamma = 1)  # The default behaviour is rbf or Radial Basis Function Kernel\n",
    "# The gamma hyperparameter controls the smoothness of the boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SGD Classifier**\n",
    "\n",
    "- Stochastic Gradient Descent Classifier\n",
    "- scales well to large data sets\n",
    "- Hyperparameter is called alpha instead of C where alpha is 1/alpha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2022.10.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
